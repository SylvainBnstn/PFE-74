import matplotlib.pyplot as plt

import Model_DQN_update
import testwithdata
from Environment import unit_cost 
from Eps_Greedy_Policy import EpsilonGreedyPolicy
import numpy as np
import Environment


###############################################################################
# Import des variables et fonctions
target_net = Model_DQN_update.target_net
to_tensor = Model_DQN_update.to_tensor
to_tensor_long = Model_DQN_update.to_tensor_long
env_step = Model_DQN_update.env_step
price_grid = Model_DQN_update.price_grid
profit_response = Model_DQN_update.profit_response
policy = EpsilonGreedyPolicy()
state_dim = Model_DQN_update.state_dim

#crochet 2 pour isoler 3 colonne convertit en array
data_test = testwithdata.get_data()[2].to_numpy()
T = 100

def env_initial_test_state(data):
    state = np.repeat(0,2*state_dim)
    state[0]= data
    return state
###############################################################################
# Test

def dqn_test(initial_state):
    
    state_test = initial_state
    #reward_trace_test = []
    #p_test = [state_test[0]]
    #for t in range(T):
    
    # Select and perform an action
    q_values_test = target_net(to_tensor(state_test))
    action_test = policy.select_action(q_values_test.detach().numpy())

 

    next_state_test, reward_test = env_step(state_test, action_test)

 

    # Move to the next state
    state_test = next_state_test

 

    return reward_test,price_grid[action_test],state_test

def cumul_reward(reward_trace_test,p_test):
####################################################
# Compute total reward 
    somme_algo_test = sum(reward_trace_test)
    price = data_test[:T,1]
    booked = data_test[:T,2]
    reward_from_data = booked * (price - unit_cost) 
    somme_data_test = sum(reward_from_data) 
    
    return somme_algo_test,somme_data_test,reward_from_data

reward_trace_test,p_test = dqn_test()
somme_algo_test,somme_data_test,reward_from_data=cumul_reward(reward_trace_test,p_test)


def plot_price(p_test,data_test):
    
    #comparaison prix généré vs prix des data
    ### compare price generated by the DQN and price from data 
    fig = plt.figure(figsize=(16, 10))
    plt.title("Price generated VS Price data")
    plt.plot(p_test , label = " Price ")
    plt.plot(data_test[:T,1],label="Data Price")
    plt.legend()
    plt.grid()

plot_price(p_test,data_test)


def plot_reward(reward_trace_test,reward_from_data):
    
    fig = plt.figure(figsize=(16, 10))
    plt.title("Reward")
    
    plt.plot(reward_trace_test,label="Reward_per_month_from_algo")
    plt.plot(reward_from_data,label="Reward from data")
    plt.legend()
    plt.grid()
    plt.show()

plot_reward(reward_trace_test,reward_from_data)
print("profit algo",somme_algo_test/T)
print("profit data",somme_data_test/T)
print("###########################################################################")
###############################################################################
# Analysis of price & demand from algo vs data

def analysis_price_demand(p_test):
    d_0 = Environment.d_0
    k = Environment.k
    a = Environment.a_q
    b = Environment.b_q
    d_t = Environment.d_t
    dt=[15]#demande init à 15 (sachant max=30)
    for i in range(1,len(p_test)):
        dt.append(d_t(p_test[i],p_test[i-1], d_0, k, a, b))
    
    fig = plt.figure(figsize=(16, 10))
    plt.title("Price_Demand/time_step")
    
    plt.plot(data_test[:T,1],label="Price")#prix
    plt.plot(data_test[:T,2],label="Demand")#demande
    plt.legend()
    plt.grid()
    plt.show()
    
    fig = plt.figure(figsize=(16, 10))
    plt.title("Pirce/Demand with Algo throught months")
    plt.plot(p_test,label="Price ")#prix
    plt.plot(dt,label="Demand")#demande
    plt.legend()
    plt.grid()
    plt.show()

analysis_price_demand(p_test)
